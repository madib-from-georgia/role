#!/usr/bin/env python3
"""
Ð£Ñ‚Ð¸Ð»Ð¸Ñ‚Ð° Ð´Ð»Ñ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð° JSON Ð»Ð¾Ð³Ð¾Ð² NLP Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
"""

import json
import sys
from pathlib import Path
from datetime import datetime
from loguru import logger

def view_latest_logs():
    """ÐŸÑ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ñ… Ð»Ð¾Ð³Ð¾Ð²"""
    
    logs_dir = Path(__file__).parent.parent / "logs"
    
    if not logs_dir.exists():
        logger.error(f"ÐŸÐ°Ð¿ÐºÐ° Ð»Ð¾Ð³Ð¾Ð² Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°: {logs_dir}")
        return
    
    print("=" * 60)
    print("ðŸ“Š NLP Analysis Logs Viewer")
    print("=" * 60)
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ latest Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ
    latest_dir = logs_dir / "latest"
    if latest_dir.exists():
        print(f"\nðŸ“ Latest Results ({latest_dir}):")
        
        latest_global = latest_dir / "latest.json"
        if latest_global.exists():
            print(f"\nðŸ”¥ Ð¡Ð°Ð¼Ñ‹Ð¹ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð·:")
            show_analysis_summary(latest_global)
        
        # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð²ÑÐµ latest Ñ„Ð°Ð¹Ð»Ñ‹
        latest_files = sorted(latest_dir.glob("*_latest.json"))
        if latest_files:
            print(f"\nðŸ“š ÐŸÐ¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ ÐºÐ½Ð¸Ð³Ð°Ð¼:")
            for file in latest_files:
                book_name = file.stem.replace("_latest", "")
                print(f"\nðŸ“– {book_name}:")
                show_analysis_summary(file)
    
    # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð²ÑÐµÑ… Ð¿Ð°Ð¿Ð¾Ðº
    print(f"\nðŸ“‚ Ð’ÑÐµ Ð¿Ð°Ð¿ÐºÐ¸ Ñ Ð»Ð¾Ð³Ð°Ð¼Ð¸:")
    book_dirs = [d for d in logs_dir.iterdir() if d.is_dir() and d.name != "latest"]
    
    for book_dir in sorted(book_dirs):
        print(f"\nðŸ“ {book_dir.name}/")
        
        # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð²ÑÐµ Ñ„Ð°Ð¹Ð»Ñ‹ Ð² Ð¿Ð°Ð¿ÐºÐµ
        json_files = sorted(book_dir.glob("*.json"), key=lambda x: x.stat().st_mtime, reverse=True)
        
        for i, file in enumerate(json_files):
            file_time = datetime.fromtimestamp(file.stat().st_mtime)
            prefix = "  ðŸ”¥" if file.name.endswith("_latest.json") else "  ðŸ“„"
            print(f"{prefix} {file.name} ({file_time.strftime('%Y-%m-%d %H:%M:%S')})")
            
            # Ð”Ð»Ñ latest Ñ„Ð°Ð¹Ð»Ð° Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ ÐºÑ€Ð°Ñ‚ÐºÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ
            if file.name.endswith("_latest.json"):
                show_analysis_summary(file, indent="      ")

def show_analysis_summary(file_path: Path, indent: str = "    "):
    """ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ ÐºÑ€Ð°Ñ‚ÐºÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾Ð± Ð°Ð½Ð°Ð»Ð¸Ð·Ðµ"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        metadata = data.get("metadata", {})
        stats = data.get("extraction_stats", {})
        characters = data.get("characters", [])
        
        print(f"{indent}ðŸ“„ Ð¤Ð°Ð¹Ð»: {metadata.get('filename', 'Unknown')}")
        print(f"{indent}â±ï¸  Ð’Ñ€ÐµÐ¼Ñ: {metadata.get('timestamp', 'Unknown')}")
        print(f"{indent}ðŸŽ­ ÐŸÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹: {len(characters)}")
        print(f"{indent}ðŸ“Š ÐœÐµÑ‚Ð¾Ð´: {stats.get('method_used', 'Unknown')}")
        print(f"{indent}âš¡ Ð’Ñ€ÐµÐ¼Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸: {metadata.get('processing_time', 0):.2f}s")
        
        if characters:
            # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ñ‚Ð¾Ð¿-3 Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹ Ð¿Ð¾ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸
            top_chars = sorted(characters, key=lambda x: x.get('importance_score', 0), reverse=True)[:3]
            print(f"{indent}ðŸŒŸ Ð¢Ð¾Ð¿ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð¸:")
            for char in top_chars:
                score = char.get('importance_score', 0)
                name = char.get('name', 'Unknown')
                print(f"{indent}   â€¢ {name} ({score:.2f})")
        
    except Exception as e:
        print(f"{indent}âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ñ„Ð°Ð¹Ð»Ð°: {e}")

def view_specific_analysis(book_name: str):
    """ÐŸÑ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð¹ ÐºÐ½Ð¸Ð³Ð¸"""
    
    logs_dir = Path(__file__).parent.parent / "logs"
    book_dir = logs_dir / book_name
    
    if not book_dir.exists():
        logger.error(f"ÐŸÐ°Ð¿ÐºÐ° Ð´Ð»Ñ ÐºÐ½Ð¸Ð³Ð¸ '{book_name}' Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
        available = [d.name for d in logs_dir.iterdir() if d.is_dir() and d.name != "latest"]
        if available:
            print(f"Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ ÐºÐ½Ð¸Ð³Ð¸: {', '.join(available)}")
        return
    
    print(f"ðŸ“– Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°: {book_name}")
    print("=" * 60)
    
    latest_file = book_dir / "nlp_result_latest.json"
    if latest_file.exists():
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿Ð¾Ð»Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ
        print("ðŸ“Š ÐœÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ:")
        for key, value in data.get("metadata", {}).items():
            print(f"  {key}: {value}")
        
        print("\nðŸ“ˆ Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ:")
        for key, value in data.get("extraction_stats", {}).items():
            print(f"  {key}: {value}")
        
        print(f"\nðŸŽ­ ÐŸÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð¸ ({len(data.get('characters', []))}):")
        for char in data.get("characters", []):
            name = char.get('name', 'Unknown')
            score = char.get('importance_score', 0)
            mentions = char.get('mentions_count', 0)
            source = char.get('source', 'Unknown')
            print(f"  â€¢ {name}")
            print(f"    Ð’Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ: {score:.2f}, Ð£Ð¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ð¹: {mentions}, Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº: {source}")
            if char.get('description'):
                print(f"    ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ: {char['description']}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        view_specific_analysis(sys.argv[1])
    else:
        view_latest_logs()
