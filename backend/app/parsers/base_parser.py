"""
–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ —Ñ–∞–π–ª–æ–≤.
"""

from abc import ABC, abstractmethod
from typing import Dict, Any
from pathlib import Path
import logging

from .content_models import StructuredContent, ContentAnalyzer

logger = logging.getLogger(__name__)


class BaseParser(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–µ—Ä–æ–≤ —Ñ–∞–π–ª–æ–≤."""
    
    def __init__(self):
        self.supported_extensions = []
        self.max_file_size = 50 * 1024 * 1024  # 50MB –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    @abstractmethod
    def parse_file(self, file_path: str) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞ (legacy –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏).
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        pass
    
    @abstractmethod
    def parse_to_structured_content(self, file_path: str) -> StructuredContent:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç.
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            –û–±—ä–µ–∫—Ç StructuredContent —Å —Ä–∞–∑–æ–±—Ä–∞–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        """
        pass
    
    @abstractmethod
    def validate_file(self, file_path: str) -> bool:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞.
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            True –µ—Å–ª–∏ —Ñ–∞–π–ª –≤–∞–ª–∏–¥–µ–Ω, False –∏–Ω–∞—á–µ
        """
        pass
    
    def _validate_basic(self, file_path: str) -> bool:
        """–ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞."""
        try:
            file_path_obj = Path(file_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
            if not file_path_obj.exists():
                logger.error(f"–§–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {file_path}")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            file_size = file_path_obj.stat().st_size
            if file_size > self.max_file_size:
                logger.error(f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {file_size} –±–∞–π—Ç (–º–∞–∫—Å–∏–º—É–º {self.max_file_size})")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
            if self.supported_extensions and file_path_obj.suffix.lower() not in self.supported_extensions:
                logger.warning(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {file_path_obj.suffix}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –ø—É—Å—Ç–æ–π
            if file_size == 0:
                logger.error("–§–∞–π–ª –ø—É—Å—Ç–æ–π")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –±–∞–∑–æ–≤–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return False
    
    def _extract_basic_metadata(self, file_path: str, content: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö."""
        file_path_obj = Path(file_path)
        
        return {
            'filename': file_path_obj.name,
            'file_size': file_path_obj.stat().st_size if file_path_obj.exists() else 0,
            'character_count': len(content),
            'word_count': len(content.split()),
            'line_count': len(content.split('\n')),
            'format': file_path_obj.suffix.lower().lstrip('.')
        }
    
    def _normalize_play_content(self, raw_content: str) -> str:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—ã—Ä–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø—å–µ—Å—ã –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞.
        –ü—Ä–∏–≤–æ–¥–∏—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –ø—å–µ—Å –∫ –µ–¥–∏–Ω–æ–º—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç—É.
        """
        import re
        
        if not raw_content:
            return ""
        
        # 1. –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
        content = raw_content.strip()
        
        # 2. –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ (–æ—Å—Ç–∞–≤–ª—è–µ–º –º–∞–∫—Å–∏–º—É–º 2 –ø–æ–¥—Ä—è–¥)
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        # 3. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–µ–π—Å—Ç–≤–∏–π –∏ —è–≤–ª–µ–Ω–∏–π
        content = self._normalize_act_headers(content)
        
        # 4. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∏–∞–ª–æ–≥–∏ –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
        content = self._normalize_dialogue_format(content)
        
        # 5. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–ø–∏—Å–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
        content = self._normalize_character_lists(content)
        
        # 6. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Ü–µ–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–º–∞—Ä–∫–∏
        content = self._normalize_stage_directions(content)
        
        # 7. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        content = self._final_cleanup(content)
        
        return content

    def _normalize_act_headers(self, content: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–µ–π—Å—Ç–≤–∏–π –∏ —è–≤–ª–µ–Ω–∏–π"""
        import re
        lines = content.split('\n')
        normalized_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                normalized_lines.append('')
                continue
                
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–µ–π—Å—Ç–≤–∏–π
            if re.match(r'(?i)(–¥–µ–π—Å—Ç–≤–∏–µ|–∞–∫—Ç)\s*(–ø–µ—Ä–≤–æ–µ|–≤—Ç–æ—Ä–æ–µ|—Ç—Ä–µ—Ç—å–µ|—á–µ—Ç–≤–µ—Ä—Ç–æ–µ|–ø—è—Ç–æ–µ|\d+)', line):
                line = re.sub(r'\s+', ' ', line).title()
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —è–≤–ª–µ–Ω–∏–π
            elif re.match(r'(?i)(—è–≤–ª–µ–Ω–∏–µ|—Å—Ü–µ–Ω–∞)\s*(–ø–µ—Ä–≤–æ–µ|–≤—Ç–æ—Ä–æ–µ|—Ç—Ä–µ—Ç—å–µ|\d+)', line):
                line = re.sub(r'\s+', ' ', line).title()
            
            normalized_lines.append(line)
        
        return '\n'.join(normalized_lines)

    def _normalize_dialogue_format(self, content: str) -> str:
        """
        –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∏–∞–ª–æ–≥–æ–≤ –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É: "–ò–ú–Ø: —Ç–µ–∫—Å—Ç"
        """
        import re
        lines = content.split('\n')
        normalized_lines = []
        
        for line in lines:
            line = line.strip()
            
            if not line:
                normalized_lines.append('')
                continue
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∏–∞–ª–æ–≥–æ–≤
            dialogue_patterns = [
                # "–ò–ú–Ø (–¥–µ–π—Å—Ç–≤–∏–µ). –¢–µ–∫—Å—Ç" -> "–ò–ú–Ø (–¥–µ–π—Å—Ç–≤–∏–µ): –¢–µ–∫—Å—Ç" (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ –≤ —Å–∫–æ–±–∫–∞—Ö)
                (r'^([–ê-–Ø–Å][–ê-–Ø–Å–∞-—è—ë\s]+?\s*\([^)]+\))\s*\.\s*(.+)$', r'\1: \2'),
                
                # "–ò–ú–Ø. –¢–µ–∫—Å—Ç" -> "–ò–ú–Ø: –¢–µ–∫—Å—Ç" (–≤—Å–µ–≥–¥–∞ —Å—Ç–∞–≤–∏–º —Ä–æ–≤–Ω–æ –æ–¥–∏–Ω –ø—Ä–æ–±–µ–ª)
                (r'^([–ê-–Ø–Å][–ê-–Ø–Å–∞-—è—ë\s]+?)\s*\.\s*(.+)$', r'\1: \2'),
                
                # "‚Äî –¢–µ–∫—Å—Ç, ‚Äî –≥–æ–≤–æ—Ä–∏—Ç –ò–ú–Ø." -> "–ò–ú–Ø: –¢–µ–∫—Å—Ç"
                (r'^‚Äî\s*(.+?),?\s*‚Äî\s*([–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+)*)\.$', r'\2: \1'),
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–≤–æ–µ—Ç–æ—á–∏—è (—É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã)
                (r'^([–ê-–Ø–Å][–ê-–Ø–Å–∞-—è—ë\s]+?)\s*:\s*(.+)$', r'\1: \2'),
            ]
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern, replacement in dialogue_patterns:
                if re.match(pattern, line):
                    line = re.sub(pattern, replacement, line)
                    break
            
            normalized_lines.append(line)
        
        return '\n'.join(normalized_lines)

    def _normalize_character_lists(self, content: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–∏—Å–∫–æ–≤ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π"""
        import re
        lines = content.split('\n')
        normalized_lines = []
        in_character_list = False
        
        for line in lines:
            line = line.strip()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞—á–∞–ª–æ —Å–ø–∏—Å–∫–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
            if re.search(r'(?i)(–¥–µ–π—Å—Ç–≤—É—é—â–∏–µ\s+–ª–∏—Ü–∞|–ø–µ—Ä—Å–æ–Ω–∞–∂–∏|–ª–∏—Ü–∞|–¥—Ä–∞–º–∞—Ç–∏—á–µ—Å–∫–∏–µ\s+–ª–∏—Ü–∞)', line):
                in_character_list = True
                line = "–î–ï–ô–°–¢–í–£–Æ–©–ò–ï –õ–ò–¶–ê:"
                normalized_lines.append('')
                normalized_lines.append(line)
                normalized_lines.append('')
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —Å–ø–∏—Å–∫–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
            if in_character_list and (
                re.search(r'(?i)(–¥–µ–π—Å—Ç–≤–∏–µ|–∞–∫—Ç|—è–≤–ª–µ–Ω–∏–µ|—Å—Ü–µ–Ω–∞)', line) or
                line.startswith('–î–µ–π—Å—Ç–≤–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç')
            ):
                in_character_list = False
                normalized_lines.append('')
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
            if in_character_list and line:
                # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Ç–æ—á–∫–∏ –≤ –∫–æ–Ω—Ü–µ
                line = re.sub(r'\s+', ' ', line).rstrip('.')
                
            normalized_lines.append(line)
        
        return '\n'.join(normalized_lines)

    def _normalize_stage_directions(self, content: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ü–µ–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–º–∞—Ä–æ–∫"""
        import re
        lines = content.split('\n')
        normalized_lines = []
        
        for line in lines:
            original_line = line.strip()
            
            if not original_line:
                normalized_lines.append('')
                continue
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–µ–º–∞—Ä–∫–∏ –≤ —Å–∫–æ–±–∫–∞—Ö
            if original_line.startswith('(') and original_line.endswith(')'):
                # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤–Ω—É—Ç—Ä–∏ —Å–∫–æ–±–æ–∫
                inside = original_line[1:-1].strip()
                line = f"({inside})"
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ä–µ–º–∞—Ä–∫–∏ (–ü–∞—É–∑–∞, –í—Ö–æ–¥–∏—Ç, etc.)
            elif re.match(r'(?i)^(–ø–∞—É–∑–∞|–≤—Ö–æ–¥–∏—Ç|–≤—ã—Ö–æ–¥–∏—Ç|—Å–∞–¥–∏—Ç—Å—è|–≤—Å—Ç–∞–µ—Ç|–º–æ–ª—á–∞–Ω–∏–µ)\.?$', original_line):
                line = original_line.rstrip('.').title() + '.'
            
            else:
                line = original_line
                
            normalized_lines.append(line)
        
        return '\n'.join(normalized_lines)

    def _final_cleanup(self, content: str) -> str:
        """–§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        import re
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫
        lines = content.split('\n')
        lines = [line.rstrip() for line in lines]
        
        # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        while lines and not lines[0]:
            lines.pop(0)
        while lines and not lines[-1]:
            lines.pop()
        
        return '\n'.join(lines)
    
    def _save_normalization_debug_files(self, raw_content: str, normalized_content: str, file_path: str):
        """–í—Ä–µ–º–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ –∏ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
        import os
        from pathlib import Path
        from datetime import datetime
        
        print(f"================–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª—ã –æ—Ç–ª–∞–¥–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è: {file_path}")
        # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∏ –ø—É—Ç–∏
        file_name = Path(file_path).stem
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –ª–æ–≥–æ–≤ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
        logs_dir = Path(__file__).parent.parent.parent / "logs" / file_name
        logs_dir.mkdir(parents=True, exist_ok=True)
        
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        raw_file = logs_dir / f"01_raw_content_{timestamp}.txt"
        with open(raw_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("–ò–°–•–û–î–ù–´–ô –ö–û–ù–¢–ï–ù–¢ (–î–û –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–ò)\n")
            f.write("=" * 80 + "\n\n")
            f.write(raw_content)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        normalized_file = logs_dir / f"02_normalized_content_{timestamp}.txt"
        with open(normalized_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("–ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù–ù–´–ô –ö–û–ù–¢–ï–ù–¢ (–ü–û–°–õ–ï –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–ò)\n")
            f.write("=" * 80 + "\n\n")
            f.write(normalized_content)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison_file = logs_dir / f"03_comparison_{timestamp}.txt"
        with open(comparison_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("–°–†–ê–í–ù–ï–ù–ò–ï: –î–û –ò –ü–û–°–õ–ï –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–ò\n")
            f.write("=" * 80 + "\n\n")
            
            f.write("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n")
            f.write(f"   –ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {len(raw_content)} —Å–∏–º–≤–æ–ª–æ–≤\n")
            f.write(f"   –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {len(normalized_content)} —Å–∏–º–≤–æ–ª–æ–≤\n")
            f.write(f"   –†–∞–∑–Ω–∏—Ü–∞: {len(normalized_content) - len(raw_content)} —Å–∏–º–≤–æ–ª–æ–≤\n")
            raw_lines_count = len(raw_content.split('\n'))
            norm_lines_count = len(normalized_content.split('\n'))
            f.write(f"   –ò—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {raw_lines_count}\n")
            f.write(f"   –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {norm_lines_count}\n\n")
            
            f.write("üîç –ü–ï–†–í–´–ï 50 –°–¢–†–û–ö - –°–†–ê–í–ù–ï–ù–ò–ï:\n")
            f.write("-" * 80 + "\n")
            
            raw_lines = raw_content.split('\n')
            norm_lines = normalized_content.split('\n')
            
            max_lines = min(50, max(len(raw_lines), len(norm_lines)))
            
            for i in range(max_lines):
                f.write(f"–°—Ç—Ä–æ–∫–∞ {i+1:3d}:\n")
                
                if i < len(raw_lines):
                    f.write(f"  –î–û:    '{raw_lines[i]}'\n")
                else:
                    f.write(f"  –î–û:    [–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç]\n")
                
                if i < len(norm_lines):
                    f.write(f"  –ü–û–°–õ–ï: '{norm_lines[i]}'\n")
                else:
                    f.write(f"  –ü–û–°–õ–ï: [–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç]\n")
                
                f.write("\n")
        
        print(f"üóÇÔ∏è –§–∞–π–ª—ã –æ—Ç–ª–∞–¥–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {logs_dir}")
        print(f"   üìÑ –ò—Å—Ö–æ–¥–Ω—ã–π: {raw_file.name}")
        print(f"   üìÑ –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π: {normalized_file.name}")
        print(f"   üìÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: {comparison_file.name}")
    
    def _cleanup_normalization_debug_files(self, file_path: str):
        """–í—Ä–µ–º–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ –æ—Ç–ª–∞–¥–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
        import shutil
        from pathlib import Path
        
        file_name = Path(file_path).stem
        logs_dir = Path(__file__).parent.parent.parent / "logs" / file_name
        
        if logs_dir.exists():
            # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (–Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å 01_, 02_, 03_)
            for file in logs_dir.glob("0[123]_*content*.txt"):
                file.unlink()
            for file in logs_dir.glob("03_comparison*.txt"):
                file.unlink()
            print(f"üßπ –§–∞–π–ª—ã –æ—Ç–ª–∞–¥–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —É–¥–∞–ª–µ–Ω—ã –∏–∑: {logs_dir}")
        else:
            print(f"‚ö†Ô∏è –ü–∞–ø–∫–∞ –ª–æ–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {logs_dir}")
    
    def _create_structured_content_from_text(self, raw_content: str, metadata: Dict[str, Any], 
                                           file_path: str) -> StructuredContent:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ —Å—ã—Ä–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
        –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.
        """
        from .content_models import (
            ContentElement, DialogueElement, CharacterListElement,
            ContentType, StructuredContent
        )
        
        # –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –ö–û–ù–¢–ï–ù–¢–ê –ü–ï–†–ï–î –ê–ù–ê–õ–ò–ó–û–ú
        normalized_content = self._normalize_play_content(raw_content)
        
        # –í–†–ï–ú–ï–ù–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª—ã –æ—Ç–ª–∞–¥–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        self._save_normalization_debug_files(raw_content, normalized_content, file_path)
        
        elements = []
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù–ù–´–ô —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –¥–∏–∞–ª–æ–≥–æ–≤
        dialogues = ContentAnalyzer.detect_dialogue_patterns(normalized_content)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù–ù–´–ô —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Å–ø–∏—Å–∫–æ–≤ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
        character_lists = ContentAnalyzer.detect_character_lists(normalized_content)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–∏—Å–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
        for char_list in character_lists:
            start_pos = self._get_text_position_by_line(normalized_content, char_list['start_line'])
            end_pos = self._get_text_position_by_line(normalized_content, char_list['end_line'])
            
            element = CharacterListElement(
                type=ContentType.CHARACTER_LIST,
                content=char_list['section_title'],
                position=start_pos,
                length=end_pos - start_pos,
                characters=char_list['characters']
            )
            elements.append(element)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∏–∞–ª–æ–≥–∏
        processed_lines = set()
        for dialogue in dialogues:
            line_pos = self._get_text_position_by_line(normalized_content, dialogue['line_number'])
            
            element = DialogueElement(
                type=ContentType.DIALOGUE,
                content=dialogue['text'],
                position=line_pos,
                length=len(dialogue['original_line']),
                speaker=dialogue['speaker']
            )
            elements.append(element)
            processed_lines.add(dialogue['line_number'])
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–π—Å—è —Ç–µ–∫—Å—Ç –∫–∞–∫ –Ω–∞—Ä—Ä–∞—Ç–∏–≤–Ω—ã–π
        lines = normalized_content.split('\n')
        for i, line in enumerate(lines):
            if i not in processed_lines and line.strip():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤—Ö–æ–¥–∏—Ç –ª–∏ —ç—Ç–∞ —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ø–∏—Å–æ–∫ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
                in_character_list = any(
                    char_list['start_line'] <= i <= char_list['end_line']
                    for char_list in character_lists
                )
                
                if not in_character_list:
                    line_pos = self._get_text_position_by_line(normalized_content, i)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                    content_type = self._detect_content_type(line.strip())
                    
                    element = ContentElement(
                        type=content_type,
                        content=line.strip(),
                        position=line_pos,
                        length=len(line)
                    )
                    elements.append(element)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ –ø–æ–∑–∏—Ü–∏–∏
        elements.sort(key=lambda x: x.position)
        
        return StructuredContent(
            elements=elements,
            raw_content=normalized_content,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
            metadata=metadata,
            source_file=file_path
        )
    
    def _get_text_position_by_line(self, text: str, line_number: int) -> int:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–∑–∏—Ü–∏—é —Å–∏–º–≤–æ–ª–∞ –ø–æ –Ω–æ–º–µ—Ä—É —Å—Ç—Ä–æ–∫–∏"""
        lines = text.split('\n')
        position = 0
        
        for i in range(min(line_number, len(lines))):
            if i > 0:
                position += 1  # –£—á–∏—Ç—ã–≤–∞–µ–º —Å–∏–º–≤–æ–ª –ø–µ—Ä–µ–≤–æ–¥–∞ —Å—Ç—Ä–æ–∫–∏
            position += len(lines[i])
        
        return position
    
    def _detect_content_type(self, line: str):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —Å—Ç—Ä–æ–∫–∏"""
        import re
        from .content_models import ContentType
        
        line_lower = line.lower()
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–µ–π—Å—Ç–≤–∏–π/–∞–∫—Ç–æ–≤
        if re.search(r'\b(–¥–µ–π—Å—Ç–≤–∏–µ|–∞–∫—Ç|–∫–∞—Ä—Ç–∏–Ω–∞|—Å—Ü–µ–Ω–∞|—è–≤–ª–µ–Ω–∏–µ|–ø—Ä–æ–ª–æ–≥|—ç–ø–∏–ª–æ–≥)\b', line_lower):
            return ContentType.CHAPTER_TITLE
        
        # –°—Ü–µ–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–º–∞—Ä–∫–∏ (–≤ —Å–∫–æ–±–∫–∞—Ö –∏–ª–∏ –∫—É—Ä—Å–∏–≤–µ)
        if (line.startswith('(') and line.endswith(')')) or \
           (line.startswith('[') and line.endswith(']')):
            return ContentType.STAGE_DIRECTION
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –Ω–∞—Ä—Ä–∞—Ç–∏–≤
        return ContentType.NARRATIVE
